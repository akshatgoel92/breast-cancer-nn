{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "np.random.seed(401293)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(426, 30)\n",
      "(426, 1)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "data = load_breast_cancer()\n",
    "X = data['data']\n",
    "y = data['target'].reshape(data['target'].shape[0], 1)\n",
    "\n",
    "# Divide into train and test data \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "# Print shapes as a sanity check\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization \n",
    "def initialize_parameters(n_x, L, layers_dims):\n",
    "       \n",
    "    # Store the parameter values here\n",
    "    parameters = {}\n",
    "    \n",
    "    # Initialize the  \n",
    "    X_dim_prev = n_x \n",
    "    \n",
    "    # Randomly initialize each layer\n",
    "    for l in range(L):\n",
    "        \n",
    "        parameters[\"W\" + str(l+1)] = np.random.randn(layers_dims[l], X_dim_prev)*0.01\n",
    "        parameters[\"b\" + str(l+1)] = np.zeros((layers_dims[l], 1))\n",
    "        X_dim_prev = layers_dims[l]\n",
    "    \n",
    "    # Return parameters\n",
    "    return(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward propogation\n",
    "def forward_propagation(X, L, parameters):\n",
    "    \n",
    "    # We'll store Z and A values here\n",
    "    Z = {}\n",
    "    A = {}\n",
    "    A[\"A0\"] = X\n",
    "    A_prev = X\n",
    "    \n",
    "    # Loop through each layer\n",
    "    for l in range(L): \n",
    "        \n",
    "        # Store the values we need cleanly\n",
    "        W_l = parameters[\"W\" + str(l+1)]\n",
    "        b_l = parameters[\"b\" + str(l+1)]\n",
    "        \n",
    "        # Send these values through the linear function\n",
    "        Z[\"Z\" + str(l+1)] = np.dot(W_l, A_prev) + b_l \n",
    "        \n",
    "        # Send the values to the sigmoid function\n",
    "        A[\"A\" + str(l+1)] = sigmoid(Z[\"Z\" + str(l+1)])\n",
    "        \n",
    "        # Update the layer\n",
    "        A_prev = A[\"A\" + str(l+1)]\n",
    "        \n",
    "    # Return statement\n",
    "    return(A, Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cost\n",
    "def compute_cost(AL, Y, m):\n",
    "    \n",
    "    # Compute the cost\n",
    "    cost = (-1/m) * np.sum(Y * np.log(AL) + (1-Y) * np.log(1-AL))\n",
    "    \n",
    "    # Return statement\n",
    "    return(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward propogation    \n",
    "def backward_propogation(L, m, A, AL, Y, Z, parameters):\n",
    "    \n",
    "    # Store the gradients here\n",
    "    grads = {}\n",
    "    \n",
    "    # Initial values for output layer\n",
    "    grads[\"dA\" + str(L)] = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    grads[\"dZ\" + str(L)] = grads[\"dA\" + str(L)] * sigmoid_prime(Z[\"Z\" + str(L)])\n",
    "    grads[\"dW\" + str(L)] = 1/m * np.dot(grads[\"dZ\" + str(L)], A[\"A\" + str(L-1)].T)\n",
    "    grads[\"db\" + str(L)] = 1/m * np.sum(grads[\"dZ\" + str(L)], axis = 1, keepdims = True)\n",
    "    \n",
    "    # Now send the values backwards through the net\n",
    "    for l in reversed(range(1, L)):\n",
    "        \n",
    "        # Helper calculations\n",
    "        first_term = np.dot(parameters[\"W\" + str(l+1)].T, grads[\"dZ\" + str(l+1)])\n",
    "        second_term = sigmoid_prime(Z[\"Z\" + str(l)])\n",
    "        \n",
    "        # Gradient calculations\n",
    "        grads[\"dZ\" + str(l)] = first_term*second_term\n",
    "        grads[\"dW\" + str(l)] = 1/m * np.dot(grads[\"dZ\" + str(l)], A[\"A\" + str(l-1)].T)\n",
    "        grads[\"db\" + str(l)] = 1/m * np.sum(grads[\"dZ\" + str(l)], axis = 1, keepdims = True)\n",
    "        \n",
    "    # Return statement\n",
    "    return(grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update parameters    \n",
    "def update_parameters(L, parameters, grads, learning_rate):\n",
    "    \n",
    "    for l in range(L): \n",
    "        \n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - (learning_rate*grads[\"dW\" + str(l+1)])\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - (learning_rate*grads[\"db\" + str(l+1)])\n",
    "     \n",
    "    # Update rule\n",
    "    return(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation function\n",
    "def sigmoid(Z): \n",
    "    \n",
    "    # Activate each unit of Z\n",
    "    return 1.0/(1.0 + np.exp(-Z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derivative of activation function w.r.t Z\n",
    "def sigmoid_prime(Z):\n",
    "    \n",
    "    # Store function value\n",
    "    sig = sigmoid(Z)\n",
    "    \n",
    "    # Return slope of sigmoid at Z\n",
    "    return (sig) *(1.0 - sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate no. of successes in prediction\n",
    "def calculate_binary_accuracy(AL, Y):\n",
    "\n",
    "    # Turn probabilities into predictions\n",
    "    predictions = (AL > 0.5).astype(int)\n",
    "    \n",
    "    # Total examples\n",
    "    total = Y.shape[1]\n",
    "    \n",
    "    # Compare predictions to training set\n",
    "    correct = np.sum((predictions == Y).astype(int))\n",
    "    \n",
    "    # Return accuracy\n",
    "    return(correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy on test set\n",
    "def calculate_test_accuracy(X_test, Y_test, L, parameters):\n",
    "    \n",
    "    # Send the test data through the net\n",
    "    A, Z = forward_propagation(X_test, L, parameters)\n",
    "    \n",
    "    # Calculate the last layer's activations\n",
    "    AL = A[\"A\" + str(L)]\n",
    "    \n",
    "    # Calculate the accuracy w.r.t. test data\n",
    "    test_accuracy = calculate_binary_accuracy(AL, Y_test)\n",
    "    \n",
    "    # Return statement\n",
    "    return(test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model implementation \n",
    "def nn_model(X, Y, X_test, Y_test, layers_dims, learning_rate, num_iterations):\n",
    "    \n",
    "    # Get NN architecture hyper-parameters\n",
    "    n_x = X.shape[0]\n",
    "    m = X.shape[1]\n",
    "    L = len(layers_dims)\n",
    "    \n",
    "    # Set parameters \n",
    "    parameters = initialize_parameters(n_x, L, layers_dims)\n",
    "    \n",
    "    # Execute the model\n",
    "    for iteration in range(1, num_iterations + 1):\n",
    "        \n",
    "        # Send data through the net\n",
    "        A, Z = forward_propagation(X, L, parameters)\n",
    "        \n",
    "        # Store predictions from this forward pass\n",
    "        AL = A[\"A\" + str(L)]\n",
    "                \n",
    "        # Calculate training set accuracy\n",
    "        training_accuracy = calculate_binary_accuracy(AL, Y)\n",
    "        \n",
    "        # Calculate the test set accuracy\n",
    "        testing_accuracy = calculate_test_accuracy(X_test, Y_test, L, parameters)\n",
    "        \n",
    "        # Compute costs \n",
    "        cost = compute_cost(AL, Y, m)\n",
    "        \n",
    "        # Compute gradients\n",
    "        grads = backward_propogation(L, m, A, AL, Y, Z, parameters) \n",
    "        \n",
    "        # Update the parameters for the next iteration\n",
    "        parameters = update_parameters(L, parameters, grads, learning_rate)\n",
    "        \n",
    "        # Print progress every 1000 steps\n",
    "        if iteration % 1000 == 0:\n",
    "            \n",
    "            placeholder = \"Iteration: {}, Cost: {}, Training accuracy: {}, Testing accuracy: {}\"\n",
    "            print(placeholder.format(iteration, cost, training_accuracy, testing_accuracy), end = '\\r')\n",
    "        \n",
    "        # Next step\n",
    "        iteration +=1\n",
    "     \n",
    "    # Generate predictions once this is done\n",
    "    predictions = (AL > 0.5).astype(int)\n",
    "    \n",
    "    # Return statement\n",
    "    return(cost, predictions, parameters, training_accuracy, testing_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 139000, Cost: 0.24298976842648395, Training accuracy: 0.9342723004694836, Testing accuracy: 0.9230769230769231\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-97ce1f225ba9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Test call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtest3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0march3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m.0005\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mtest1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0march1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m.0005\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtest2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0march2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m.0005\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-3f8e7af97e8e>\u001b[0m in \u001b[0;36mnn_model\u001b[0;34m(X, Y, X_test, Y_test, layers_dims, learning_rate, num_iterations)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m# Compute gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackward_propogation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m# Update the parameters for the next iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-51d1757e93b3>\u001b[0m in \u001b[0;36mbackward_propogation\u001b[0;34m(L, m, A, AL, Y, Z, parameters)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dZ\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfirst_term\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msecond_term\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dW\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mm\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dZ\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"A\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"db\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mm\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dZ\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# Return statement\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Test architectures\n",
    "arch1 = [30, 1]\n",
    "arch2 = [15, 1]\n",
    "arch3 = [15, 2, 1]\n",
    "\n",
    "# Test call\n",
    "test3 = nn_model(X_train.T, y_train.T, X_test.T, y_test.T, arch3, .0005, 200000)\n",
    "test1 = nn_model(X_train.T, y_train.T, X_test.T, y_test.T, arch1, .0005, 200000)\n",
    "test2 = nn_model(X_train.T, y_train.T, X_test.T, y_test.T, arch2, .0005, 200000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
